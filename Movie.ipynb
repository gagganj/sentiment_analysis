{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a6d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gagga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gagga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing # Import preprocessing for String-Int conversion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd7a605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  sentiment  label\n",
      "0            !    0.50000      0\n",
      "1          ! '    0.52778      0\n",
      "2         ! ''    0.50000      0\n",
      "3       ! Alas    0.44444      0\n",
      "4  ! Brilliant    0.86111      0\n"
     ]
    }
   ],
   "source": [
    "# Load in the dictionary dataset into a pandas df\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'dictionary.txt')\n",
    "dic = pd.read_csv(data_path, sep='|', header=None)\n",
    "\n",
    "# ...and do the same with the sentiment_labels data\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'sentiment_labels.txt')\n",
    "cents = pd.read_csv(data_path, sep='|')\n",
    "\n",
    "dic = dic.rename(columns={0: \"feature\", 1: \"ID\"}) # We rename the columns\n",
    "dic = dic.sort_values(by=\"ID\") # We change the order of the rows to be sorted by ID number\n",
    "#phrase_data = dic[[\"ID\"], [\"feature\"]] # We change the order of the columns and change the name of the the df\n",
    "\n",
    "# We make an array of the sentiments (already in the right order) and add it to our df\n",
    "y = np.array(cents.iloc[:, -1])\n",
    "dic.insert(2, \"sentiment\", y)\n",
    "\n",
    "phrase_data = dic.sort_index()\n",
    "phrase_data.drop(\"ID\",axis=1,inplace=True)\n",
    "phrase_data.insert(2,\"label\",0)\n",
    "\n",
    "print(phrase_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b4ebdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-b138a3be6285>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phrase_data[\"label\"][i]=2\n",
      "<ipython-input-68-b138a3be6285>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phrase_data[\"feature\"][i]= cleaned\n",
      "<ipython-input-68-b138a3be6285>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phrase_data[\"label\"][i]=4\n",
      "<ipython-input-68-b138a3be6285>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phrase_data[\"label\"][i]=3\n",
      "<ipython-input-68-b138a3be6285>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phrase_data[\"label\"][i]=1\n",
      "<ipython-input-68-b138a3be6285>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  phrase_data[\"label\"][i]=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  label\n",
      "3       [alas]      2\n",
      "4  [brilliant]      4\n",
      "5  [brilliant]      4\n",
      "6  [brilliant]      4\n",
      "7     [c, mon]      2\n"
     ]
    }
   ],
   "source": [
    "#data preprossing\n",
    "\n",
    "filler_words = set(stopwords.words('english'))\n",
    "\n",
    "#values =[[0, 0.2], [0.2, 0.4], [0.4, 0.6], [0.6, 0.8], [0.8, 1.0]]\n",
    "labels = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
    "to_drop=[]\n",
    "\n",
    "\n",
    "# relabel sentiment with corresponding sentiment + drop sentiment column\n",
    "for i in range (phrase_data.shape[0]):\n",
    "    if (phrase_data[\"sentiment\"][i]<=0.2):\n",
    "        phrase_data[\"label\"][i]=0\n",
    "    if (phrase_data[\"sentiment\"][i]>0.2) and (phrase_data[\"sentiment\"][i]<=0.4):\n",
    "        phrase_data[\"label\"][i]=1\n",
    "    if (phrase_data[\"sentiment\"][i]>0.4) and (phrase_data[\"sentiment\"][i]<=0.6):\n",
    "        phrase_data[\"label\"][i]=2\n",
    "    if (phrase_data[\"sentiment\"][i]>0.6) and (phrase_data[\"sentiment\"][i]<=0.8):\n",
    "        phrase_data[\"label\"][i]=3\n",
    "    if (phrase_data[\"sentiment\"][i]>0.8) and (phrase_data[\"sentiment\"][i]<=1):\n",
    "        phrase_data[\"label\"][i]=4\n",
    "\n",
    "    # 'clean' phrases: remove numbers, punctuation and filler words\n",
    "    phrase = phrase_data[\"feature\"][i]\n",
    "    cleaned = re.sub(r'[^\\w]', ' ', phrase) #remove all special characters \n",
    "    cleaned = re.sub(r'[\\d]', ' ',cleaned)  #remove all numbers \n",
    "    if (cleaned.replace(\" \",\"\")==\"\"):\n",
    "        to_drop.append(i)\n",
    "        cleaned=\"\"\n",
    "    else:\n",
    "        cleaned = word_tokenize(cleaned.lower()) #tokenise for bag of words\n",
    "        cleaned = [w for w in cleaned if w not in filler_words] # #remove all filler words\n",
    "    phrase_data[\"feature\"][i]= cleaned \n",
    "\n",
    "# remove unnecessary data\n",
    "phrase_data.drop(\"sentiment\",axis=1,inplace=True) \n",
    "phrase_data.drop(to_drop,axis=0,inplace=True)\n",
    "df = phrase_data[~phrase_data.astype(str).duplicated()]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(\"index\",axis=1,inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "#save this as file - takes 13 mins to run\n",
    "df.to_pickle(\"clean_phrase_data.pkl\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "afa6c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         3\n",
      "4         7\n",
      "         ..\n",
      "152013    2\n",
      "152014    7\n",
      "152015    1\n",
      "152016    0\n",
      "152017    3\n",
      "Length: 152018, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert phrases to numerical representation: bag of words\n",
    "phrase_data = pd.read_pickle(\"clean_phrase_data.pkl\") \n",
    "\n",
    "#split phrase datasets to x and y\n",
    "X = phrase_data[\"feature\"].copy()\n",
    "y = phrase_data[\"label\"].copy()\n",
    "\n",
    "\n",
    "#create word vector \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizerfit = vectorizer.fit_transform(X.astype(str))\n",
    "names = vectorizer.get_feature_names()\n",
    "count_array = vectorizerfit.toarray()\n",
    "vec = pd.DataFrame(data=count_array,columns=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "270bf75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106412, 17719) (45606, 17719) (106412,)\n"
     ]
    }
   ],
   "source": [
    "#split test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(vec, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "print(x_train.shape,x_test.shape,y_train.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4afc1b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6085497876179379\n",
      "0.5052405385256326\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "print(clf.score(x_train,y_train))\n",
    "print(clf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17350e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above not great, maybe add in a regulariser?\n",
    "\n",
    "#confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "181b4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest - grid search for optimal decision tree parameters -maybe try dif criterion\n",
    "rf_dic={\n",
    "    \"n_estimators\":[10,50,200,500],\n",
    "    \"max_features\": [\"sqrt\",\"log2\"],\n",
    "    \"criterion\": [\"gini\"],\n",
    "    \"max_depth\": [4,8,30]\n",
    "    }\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "grid_search = GridSearchCV(estimator=rf,param_grid=rf_dic,cv=3)\n",
    "grid_search.fit(x_train,y_train)\n",
    "\n",
    "print(\"the best parameters are: \" +str(grid_search.best_params_))\n",
    "\n",
    "print(\"accuracy: \" + str(grid_search.best_estimator_.score(x_test, y_test)*100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a409bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "matrix = confusion_matrix(y_test,grid_search.best_estimator_.predict(x_test), normalize='true')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(matrix, annot=True) \n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('predicted')\n",
    "ax.set_ylabel('true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103fb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log reg. #nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f6b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

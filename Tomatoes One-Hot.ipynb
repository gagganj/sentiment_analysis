{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "#import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing # Import preprocessing for String-Int conversion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dictionary dataset into a pandas df\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'dictionary.txt')\n",
    "dic = pd.read_csv(data_path, sep='|', header=None)\n",
    "\n",
    "# ...and do the same with the sentiment_labels data\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'sentiment_labels.txt')\n",
    "cents = pd.read_csv(data_path, sep='|')\n",
    "\n",
    "dic = dic.rename(columns={0: \"feature\", 1: \"ID\"})# We rename the columns\n",
    "# dic = dic.rename(index={range(239232)})\n",
    "dic = dic.sort_values(by=\"ID\") # We change the order of the rows to be sorted by ID number\n",
    "movie_data = dic[[\"ID\", \"feature\"]] # We change the order of the columns and change the name of the the df\n",
    "\n",
    "# We make an array of the sentiments (already in the right order) and add it to our df\n",
    "y = np.array(cents.iloc[:, -1])\n",
    "movie_data.insert(2, \"sentiment\", y)\n",
    "\n",
    "phrase_data = movie_data.sort_index()\n",
    "phrase_data.drop(\"ID\",axis=1,inplace=True)\n",
    "phrase_data.insert(2,\"label\",0)\n",
    "\n",
    "print(phrase_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We create a smaller subsample of the dataset - to speed up the computation when working on our code\n",
    "\n",
    "# specify a smaller number of reviews\n",
    "small_N = 2390\n",
    "\n",
    "# choose small_N random and distinct integers between 0 and 239231\n",
    "rand = random.sample(range(239231), small_N)\n",
    "\n",
    "# find these indices in the original dictionary - and make a new array of them\n",
    "rand_sample = phrase_data.iloc[rand]\n",
    "\n",
    "rand_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6287d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can use the random sample or the full data set going forward\n",
    "\n",
    "phrases = phrase_data\n",
    "# phrases = rand_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbf024",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add correct labels based on sentiment column (uses qualities of numpy for efficiency)\n",
    "\n",
    "np_phrase = np.array(phrases) # make it a numpy array\n",
    "\n",
    "# create a series of boolean masks\n",
    "vpos = (0.8 < np_phrase[:, 1]).astype(int)\n",
    "pos =  (0.6 < np_phrase[:, 1]).astype(int)\n",
    "ntrl = (0.4 < np_phrase[:, 1]).astype(int)\n",
    "neg = (0.2 < np_phrase[:, 1]).astype(int)\n",
    "vneg = (0 <= np_phrase[:, 1]).astype(int)\n",
    "\n",
    "# add the masks together to get the correct label numbers for each review based on sentiment value\n",
    "h = vneg + neg + ntrl + pos + vpos - 1\n",
    "\n",
    "# update the array with our new values\n",
    "np_phrase[:, 2] = h\n",
    "\n",
    "# change back to a pandas\n",
    "phrases = pd.DataFrame(np_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "filler_words = set(stopwords.words('english'))\n",
    "\n",
    "#values =[[0, 0.2], [0.2, 0.4], [0.4, 0.6], [0.6, 0.8], [0.8, 1.0]]\n",
    "labels = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
    "to_drop = []\n",
    "\n",
    "for i in range(small_N):\n",
    "    # 'clean' phrases: remove numbers, punctuation and filler words\n",
    "    phrase = phrases.loc[i, 0]\n",
    "    cleaned = re.sub(r'[^\\w]', \" \", phrase) #remove all special characters \n",
    "    cleaned = re.sub(r'[\\d]', \" \", cleaned)  #remove all numbers \n",
    "    if (cleaned.replace(\" \",\"\")==\"\"):\n",
    "        to_drop.append(i)\n",
    "        cleaned=\"\"\n",
    "    else:\n",
    "        cleaned = word_tokenize(cleaned.lower()) #tokenise for bag of words\n",
    "        cleaned = [w for w in cleaned if w not in filler_words] # #remove all filler words\n",
    "    phrases.loc[i, 0] = cleaned \n",
    "\n",
    "# remove unnecessary data\n",
    "phrases.drop(1,axis=1,inplace=True) \n",
    "phrases.drop(to_drop,axis=0,inplace=True)\n",
    "df = phrases[~phrases.astype(str).duplicated()]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(\"index\",axis=1,inplace=True)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n",
    "#save this as file\n",
    "df.to_pickle(\"clean_rand_sample.pkl\")\n",
    "# df.to_pickle(\"clean_phrase_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeedbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert phrases to numerical representation: bag of words\n",
    "phrases = pd.read_pickle(\"clean_rand_sample.pkl\") \n",
    "\n",
    "#split phrase datasets to x and y\n",
    "X_rand = phrases.loc[:, 0].copy()\n",
    "y_rand = phrases.loc[:, 2].copy()\n",
    "\n",
    "y_rand = y_rand.astype(int)\n",
    "\n",
    "#create word vector \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizerfit = vectorizer.fit_transform(X_rand.astype(str))\n",
    "names = vectorizer.get_feature_names_out()\n",
    "count_array = vectorizerfit.toarray()\n",
    "vec = pd.DataFrame(data=count_array,columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(vec, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "print(x_train.shape,x_test.shape,y_train.ravel().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "print(clf.score(x_train,y_train))\n",
    "print(clf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#above not great, maybe add in a regulariser?\n",
    "\n",
    "#confusion matrix\n",
    "matrix = confusion_matrix(y_test,clf.predict(x_test), normalize='true')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(matrix, annot=True) \n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('predicted')\n",
    "ax.set_ylabel('true')\n",
    "# plt.savefig(\"NB_confused.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f50b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #random forest - grid search for optimal decision tree parameters -maybe try dif criterion\n",
    "# rf_dic={\n",
    "#     \"n_estimators\":[10,50,200,500],\n",
    "#     \"max_features\": [\"sqrt\",\"log2\"],\n",
    "#     \"criterion\": [\"gini\"],\n",
    "#     \"max_depth\": [4,8,30]\n",
    "#     }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=rf,param_grid=rf_dic,cv=3)\n",
    "# grid_search.fit(x_train,y_train)\n",
    "\n",
    "# print(\"the best parameters are: \" +str(grid_search.best_params_))\n",
    "\n",
    "# print(\"accuracy: \" + str(grid_search.best_estimator_.score(x_test, y_test)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf975d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #confusion matrix\n",
    "# matrix = confusion_matrix(y_test,grid_search.best_estimator_.predict(x_test), normalize='true')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# sns.heatmap(matrix, annot=True) \n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.set_yticklabels(labels)\n",
    "# ax.set_xlabel('predicted')\n",
    "# ax.set_ylabel('true')\n",
    "# # plt.savefig(\"RandForest_confused.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d68b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log reg\n",
    "\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(x_train,y_train)\n",
    "print(clf2.score(x_train,y_train))\n",
    "print(clf2.score(x_test,y_test))\n",
    "\n",
    "matrix2 = confusion_matrix(y_test,clf2.predict(x_test), normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(matrix2, annot=True) \n",
    "ax.set_xticklabels([0, 1, 2, 3, 4])\n",
    "ax.set_yticklabels([0, 1, 2, 3, 4])\n",
    "ax.set_xlabel('predicted')\n",
    "ax.set_ylabel('true')\n",
    "# plt.savefig(\"logreg_confused.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
